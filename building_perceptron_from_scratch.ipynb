{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING PERCEPTRON FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron is a simple neural network algorithm for predicting binary outputs. It is contingent on the input data being linearly seperable.\n",
    "\n",
    "In this notebook we'll look at the underlying maths of this algorithm and see how it can be implemented in Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I - BREAKING IT DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data.\n",
    "\n",
    "For now, we'll work with a single vector (row) consisting of three features (columns). We'll also specify output $y$ for this vector, this is the outcome variable we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([2, 1, 2])\n",
    "\n",
    "y = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the weight vector with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.zeros(len(data))\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the dot product.\n",
    "\n",
    "To generate a prediction, we first need to calculate the dot product. The dot product is the sum of the weight vector $w_i$ multiplied by the feature vector $x_i$.\n",
    "\n",
    "$$ \\sum^{n}_{i=1} w_i x_i $$\n",
    "\n",
    "In our case this becomes:\n",
    "\n",
    "$$ (0 \\times 2) + (0 \\times 1) + (0 \\times 2) = 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = sum(weights * data)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the numpy function ```np.dot()``` to do the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = np.dot(weights, data)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function.\n",
    "\n",
    "Next, we compare the dot product against an activation function treshold. This tells our model to predict class 1 if the dot product is larger than 1.0, otherwise predict class 0. We store this prediction in ```y_hat```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treshold = 1 # Specify treshold\n",
    "\n",
    "\n",
    "if dot > treshold:\n",
    "    y_hat = 1\n",
    "else:\n",
    "    y_hat = 0\n",
    "\n",
    "\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights.\n",
    "\n",
    "If our prediction doesn't match the actual output, we update the weights as follow:\n",
    "\n",
    "$$ w_{i}^{n+1} = w_{i}^{n} + \\eta (y_i - \\hat y_i) x_i $$\n",
    "\n",
    "Where $\\eta$ (eta) denotes learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.1, 0.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # Specify learning rate\n",
    "\n",
    "weights = weights + eta * (y - y_hat) * data # Update the weights\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These updated weights will be used in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function.\n",
    "\n",
    "To evaluate the performance of our model we'll compute the sum of squared errors (SSE) as follows:\n",
    "\n",
    "$$ \\frac{1}{2} \\sum^{n}_{i=1} (y_i - \\hat y_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = [(y - y_hat) ** 2]  # Square the difference of y - y_hat\n",
    "sse_loss = 0.5 * sum(error) # Sum the squared errors and multiply by 1/2 \n",
    "\n",
    "sse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model iterates over the dataset, learning new weights each time, we'll see the loss minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II - PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the perceptron function.\n",
    "\n",
    "We define the model function with the following parameters:\n",
    "\n",
    "```data``` - Dataset, inclusive of features.  \n",
    "```y``` - Outcome variable.  \n",
    "```treshold``` - Activation function treshold.  \n",
    "```eta``` - Learning rate.  \n",
    "```total_epochs``` - Number of times we want the model to go over the entire dataset.\n",
    "\n",
    "The function returns the final weight vector and displays the progression of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(data, y, treshold, eta, total_epochs):\n",
    "\n",
    "    current_epoch = 0                # Keep track of iterations\n",
    "    y_hat_vector = np.zeros(len(y))  # Vector for storing predictions\n",
    "    sse_loss = []                    # Vector for SSE\n",
    "    \n",
    "    weights = np.zeros(len(data[0])) # Initialize weights with zeros\n",
    "    \n",
    "    while current_epoch < total_epochs:\n",
    "        \n",
    "        for i in range(0, len(data)): # For each row in data\n",
    "            \n",
    "            # Compute the dot product\n",
    "            dot = np.dot(weights, data[i]) \n",
    "            \n",
    "            # Activation Function\n",
    "            if dot > treshold:\n",
    "                y_hat = 1\n",
    "            else:\n",
    "                y_hat = 0\n",
    "                \n",
    "            y_hat_vector[i] = y_hat # Update predictions\n",
    "            \n",
    "            # Update weights\n",
    "            weights = weights + eta * (y[i] - y_hat) * data[i]\n",
    "            \n",
    "        # Compute SSE\n",
    "        errors = (y - y_hat_vector) ** 2   # Compute the squared difference y - y_hat\n",
    "        sse_loss.append(0.5 * sum(errors)) # Append the computed loss\n",
    "        \n",
    "        # Update epoch count\n",
    "        current_epoch += 1 \n",
    "\n",
    "    return print(\"Weights: \", weights, \"SSE: \", sse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify data, output vector, and model parameters.\n",
    "\n",
    "The first column of the dataset (all $1$s) represents the bias term (intercept), the two remaining columns are features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 0, 0], \n",
    "                 [1, 0, 1],\n",
    "                 [1, 1, 0], \n",
    "                 [1, 1, 1]])\n",
    "\n",
    "y = np.array([1, 1, 1, 0])\n",
    "\n",
    "treshold = 1\n",
    "eta = 0.1\n",
    "total_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.\n",
    "\n",
    "And finally we call the perceptron function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  [ 1.3 -0.2 -0.1] SSE:  [1.5, 1.5, 2.0, 1.0, 1.5, 1.5, 1.0, 1.5, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "perceptron(data, y, treshold, eta, total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE tells us that the model converged and learned the correct weights during the 11th epoch. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:london]",
   "language": "python",
   "name": "conda-env-london-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
